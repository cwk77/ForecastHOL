{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting Traffic Volume - Validating and Importing Related Time Series Data\n",
    "**이 노트북의 원본은 https://github.com/chrisking/ForecastPOC.git 으로서, 원본 내용의 기반하에 추가 내용등을 기술한 버전 입니다.**\n",
    "\n",
    "Summary: \n",
    "* 데이타 준비\n",
    "    * Related Time Series 데이타 준비 및\n",
    "        * timestamp, temperature, rain_1h, snow_1h, clouds_all, weather, item_id 로 구성 함          * Forecast Object 생성\n",
    "    * 스키마를 생성하고 Related-Time-Series 의 데이타 셋을 생성\n",
    "    * 위에서 생성한 데이타 셋( Related-Time-Series) 과 Target-Time-Serie 을 데이타 셋 그룹으로 연결\n",
    "    * S3에 Related-Time-Series 데이타 업로드\n",
    "    * Related Time Series Data를 데이타 셋에 Import 하는 잡 생성 및 실행\n",
    "* 데이타 정보\n",
    "    * 데이타 정의\n",
    "        * Data File: Hourly Interstate 94 Westbound traffic volume for MN DoT ATR station 301, roughly midway between Minneapolis and St Paul, MN. Hourly weather features and holidays included for impacts on traffic volume.)\n",
    "    * Attribute 정보\n",
    "        * holiday: Categorical US National holidays plus regional holiday, Minnesota State Fair \n",
    "        * rain_1h: Numeric Amount in mm of rain that occurred in the hour\n",
    "        * snow_1h: Numeric Amount in mm of snow that occurred in the hour\n",
    "        * clouds_all: Numeric Percentage of cloud cover\n",
    "        * weather_main: Categorical Short textual description of the current weather\n",
    "        * weather_description: Categorical Longer textual description of the current weather\n",
    "        * date_time: DateTime Hour of the data collected in local CST time\n",
    "        * traffic_volume: Numeric Hourly I-94 ATR 301 reported westbound traffic volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating and Importing Related Time Series Data\n",
    "\n",
    "## Obtaining Your Data\n",
    "\n",
    "This will take off where you stopped regarding your target time series data. In this particular exmaple, one master file contained both the target and the related time series information. That may or may not be the case for your problem. The goal here is to produce a file that contains the following 2 required attributes:\n",
    "\n",
    "1. Timestamp - Must be of the same format and total range as the target-time series data, as well as slices of values into the dates for your forecast.\n",
    "1. Item_ID - Must exist for all the time stamps for each item in your time series dataset\n",
    "\n",
    "In addition to those attributes we are looking for variables that shift over time that are impactful in some way towards our desired goal of predicting traffic volumes.\n",
    "\n",
    "Again the data was already bundled together for us in this sample so we will skip obtaining it a second time but that is where you would start otherwise.\n",
    "\n",
    "With the data ready to go, skip the blank cell ( feel free to add to it if you need to manipulate your own data) and execute the cells to handle our imports and retrieving our stored values from the previous notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from time import sleep\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import pprint\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter\n",
    "import matplotlib.dates as mdates\n",
    "%store -r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building The Related Time Series File\n",
    "\n",
    "The challenge here is to make sure that we leave absolutely 0 entries with NaN values or the service will throw an error when building a Predictor. This is because the values must be present in order for us to make assumptions about their impact overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-01 00:00:00\n",
      "2018-09-30 23:00:00\n"
     ]
    }
   ],
   "source": [
    "related_time_series_df = target_df.copy()\n",
    "related_time_series_df.dropna()\n",
    "related_time_series_df = full_df.join(related_time_series_df, how='outer')\n",
    "cols = related_time_series_df.columns.tolist()\n",
    "related_time_series_df[cols] = related_time_series_df[cols].replace('', np.nan).ffill()\n",
    "related_time_series_df = related_time_series_df.loc['2017-09-01':]\n",
    "print (related_time_series_df.index.min())\n",
    "print (related_time_series_df.index.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see now that the data covers the range of our target time series of 2017's entire year to the end of our known data about 2018. We have not yet defined a forecast horizon yet but it is important to note here that the related data needs to cover that time span. To spoil later work, the horizon for us is 480 hours or 20 days, plenty of time with 9 months of validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly on prepping the base set of the data we validate there are no blanks or NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>holiday</th>\n",
       "      <th>temp</th>\n",
       "      <th>rain_1h</th>\n",
       "      <th>snow_1h</th>\n",
       "      <th>clouds_all</th>\n",
       "      <th>weather_main</th>\n",
       "      <th>weather_description</th>\n",
       "      <th>traffic_volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [holiday, temp, rain_1h, snow_1h, clouds_all, weather_main, weather_description, traffic_volume]\n",
       "Index: []"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "related_time_series_df[related_time_series_df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the columns and decide what we should keep:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>holiday</th>\n",
       "      <th>temp</th>\n",
       "      <th>rain_1h</th>\n",
       "      <th>snow_1h</th>\n",
       "      <th>clouds_all</th>\n",
       "      <th>weather_main</th>\n",
       "      <th>weather_description</th>\n",
       "      <th>traffic_volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-04-29 07:00:00</th>\n",
       "      <td>None</td>\n",
       "      <td>276.44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Clear</td>\n",
       "      <td>sky is clear</td>\n",
       "      <td>1397.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-18 11:00:00</th>\n",
       "      <td>None</td>\n",
       "      <td>296.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Clear</td>\n",
       "      <td>sky is clear</td>\n",
       "      <td>4916.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-22 07:00:00</th>\n",
       "      <td>None</td>\n",
       "      <td>265.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>overcast clouds</td>\n",
       "      <td>4955.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    holiday    temp  rain_1h  snow_1h  clouds_all  \\\n",
       "2018-04-29 07:00:00    None  276.44      0.0      0.0         1.0   \n",
       "2018-08-18 11:00:00    None  296.00      0.0      0.0         5.0   \n",
       "2017-12-22 07:00:00    None  265.93      0.0      0.0        90.0   \n",
       "\n",
       "                    weather_main weather_description  traffic_volume  \n",
       "2018-04-29 07:00:00        Clear        sky is clear          1397.0  \n",
       "2018-08-18 11:00:00        Clear        sky is clear          4916.0  \n",
       "2017-12-22 07:00:00       Clouds     overcast clouds          4955.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "related_time_series_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things to note here:\n",
    "\n",
    "* Holidays are not needed given this date is in the US, we can just use the Holidays feature within Forecast: https://docs.aws.amazon.com/forecast/latest/dg/API_SupplementaryFeature.html\n",
    "* Weather description seems to have more variety\n",
    "* Traffic volume will be removed here. \n",
    "* We still need to add back the item_id field.\n",
    "\n",
    "This leaves us with the following schema:\n",
    "\n",
    "* `timestamp` - The Index\n",
    "* `temp` - float\n",
    "* `rain_1h` - float\n",
    "* `snow_1h` - float\n",
    "* `clouds_all` - float\n",
    "* `weather_description` - string\n",
    "* `item_ID` - string\n",
    "\n",
    "The cell below will build that file for us.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temp</th>\n",
       "      <th>rain_1h</th>\n",
       "      <th>snow_1h</th>\n",
       "      <th>clouds_all</th>\n",
       "      <th>weather_description</th>\n",
       "      <th>item_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-09-01 00:00:00</th>\n",
       "      <td>286.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>haze</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-01 00:00:00</th>\n",
       "      <td>286.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mist</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-01 01:00:00</th>\n",
       "      <td>285.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>haze</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-01 01:00:00</th>\n",
       "      <td>285.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mist</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-01 02:00:00</th>\n",
       "      <td>285.57</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>haze</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       temp  rain_1h  snow_1h  clouds_all weather_description  \\\n",
       "2017-09-01 00:00:00  286.22      0.0      0.0         1.0                haze   \n",
       "2017-09-01 00:00:00  286.22      0.0      0.0         1.0                mist   \n",
       "2017-09-01 01:00:00  285.40      0.0      0.0         1.0                haze   \n",
       "2017-09-01 01:00:00  285.40      0.0      0.0         1.0                mist   \n",
       "2017-09-01 02:00:00  285.57      0.0      0.0         1.0                haze   \n",
       "\n",
       "                    item_ID  \n",
       "2017-09-01 00:00:00       1  \n",
       "2017-09-01 00:00:00       1  \n",
       "2017-09-01 01:00:00       1  \n",
       "2017-09-01 01:00:00       1  \n",
       "2017-09-01 02:00:00       1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Restrict the columns to timestamp, traffic_volume\n",
    "related_time_series_df = related_time_series_df[['temp', 'rain_1h', 'snow_1h', 'clouds_all', 'weather_description']]\n",
    "# Add in item_id\n",
    "related_time_series_df['item_ID'] = \"1\"\n",
    "# Validate the structure\n",
    "related_time_series_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save it off as a file:\n",
    "related_time_series_filename = \"related_time_series.csv\"\n",
    "related_time_series_path = data_dir + \"/\" + related_time_series_filename\n",
    "related_time_series_df.to_csv(related_time_series_path, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Related Data to the DatasetGroup\n",
    "\n",
    "Next we are going to create a related-time-series dataset, then add it to our dataset group and finally import our information and validate that it looks good. We will also delete this dataset import after we are done so that the first models do not yet receive any extra info from the related data.\n",
    "\n",
    "You can of course to not delete and get started right away with related data informed models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "session = boto3.Session(region_name=region)\n",
    "forecast = session.client(service_name='forecast')\n",
    "forecast_query = session.client(service_name='forecastquery')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload Related File\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(related_time_series_filename).upload_file(related_time_series_path)\n",
    "related_s3DataPath = \"s3://\"+bucket_name+\"/\"+related_time_series_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the schema of your dataset here. Make sure the order of columns matches the raw data files.\n",
    "related_schema ={\n",
    "   \"Attributes\":[\n",
    "      {\n",
    "         \"AttributeName\":\"timestamp\",\n",
    "         \"AttributeType\":\"timestamp\"\n",
    "      },\n",
    "      {\n",
    "         \"AttributeName\":\"temperature\",\n",
    "         \"AttributeType\":\"float\"\n",
    "      },\n",
    "       {\n",
    "         \"AttributeName\":\"rain_1h\",\n",
    "         \"AttributeType\":\"float\"\n",
    "      },\n",
    "       {\n",
    "         \"AttributeName\":\"snow_1h\",\n",
    "         \"AttributeType\":\"float\"\n",
    "      },\n",
    "       {\n",
    "         \"AttributeName\":\"clouds_all\",\n",
    "         \"AttributeType\":\"float\"\n",
    "      },\n",
    "       {\n",
    "         \"AttributeName\":\"weather\",\n",
    "         \"AttributeType\":\"string\"\n",
    "      },\n",
    "      {\n",
    "         \"AttributeName\":\"item_id\",\n",
    "         \"AttributeType\":\"string\"\n",
    "      }\n",
    "   ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_DSN = datasetName + \"_related\"\n",
    "response=forecast.create_dataset(\n",
    "                    Domain=\"CUSTOM\",\n",
    "                    DatasetType='RELATED_TIME_SERIES',\n",
    "                    DatasetName=related_DSN,\n",
    "                    DataFrequency=DATASET_FREQUENCY, \n",
    "                    Schema = related_schema\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DatasetArn': 'arn:aws:forecast:ap-northeast-2:180246855724:dataset/forecast_poc_cwk_ds_related',\n",
       " 'DatasetName': 'forecast_poc_cwk_ds_related',\n",
       " 'Domain': 'CUSTOM',\n",
       " 'DatasetType': 'RELATED_TIME_SERIES',\n",
       " 'DataFrequency': 'H',\n",
       " 'Schema': {'Attributes': [{'AttributeName': 'timestamp',\n",
       "    'AttributeType': 'timestamp'},\n",
       "   {'AttributeName': 'temperature', 'AttributeType': 'float'},\n",
       "   {'AttributeName': 'rain_1h', 'AttributeType': 'float'},\n",
       "   {'AttributeName': 'snow_1h', 'AttributeType': 'float'},\n",
       "   {'AttributeName': 'clouds_all', 'AttributeType': 'float'},\n",
       "   {'AttributeName': 'weather', 'AttributeType': 'string'},\n",
       "   {'AttributeName': 'item_id', 'AttributeType': 'string'}]},\n",
       " 'EncryptionConfig': {},\n",
       " 'Status': 'ACTIVE',\n",
       " 'CreationTime': datetime.datetime(2020, 3, 22, 11, 57, 0, 354000, tzinfo=tzlocal()),\n",
       " 'LastModificationTime': datetime.datetime(2020, 3, 22, 11, 57, 0, 354000, tzinfo=tzlocal()),\n",
       " 'ResponseMetadata': {'RequestId': 'a70384f6-0637-4aa3-8b02-1049e66f7dae',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'content-type': 'application/x-amz-json-1.1',\n",
       "   'date': 'Sun, 22 Mar 2020 11:57:00 GMT',\n",
       "   'x-amzn-requestid': 'a70384f6-0637-4aa3-8b02-1049e66f7dae',\n",
       "   'content-length': '732',\n",
       "   'connection': 'keep-alive'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "related_datasetArn = response['DatasetArn']\n",
    "forecast.describe_dataset(DatasetArn=related_datasetArn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attach the Dataset to the Dataset Group:\n",
    "**Important: Attach the target data set and releated data set to the dataset group**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '38c8b963-6fd2-4e0f-b1d3-4eacec4c2395',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'content-type': 'application/x-amz-json-1.1',\n",
       "   'date': 'Sun, 22 Mar 2020 11:58:36 GMT',\n",
       "   'x-amzn-requestid': '38c8b963-6fd2-4e0f-b1d3-4eacec4c2395',\n",
       "   'content-length': '2',\n",
       "   'connection': 'keep-alive'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast.update_dataset_group(DatasetGroupArn=datasetGroupArn, DatasetArns=[target_datasetArn, related_datasetArn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetImportJobName = 'DSIMPORT_JOB_RELATEDPOC'\n",
    "related_ds_import_job_response=forecast.create_dataset_import_job(DatasetImportJobName=datasetImportJobName,\n",
    "                                                          DatasetArn=related_datasetArn,\n",
    "                                                          DataSource= {\n",
    "                                                              \"S3Config\" : {\n",
    "                                                                 \"Path\":related_s3DataPath,\n",
    "                                                                 \"RoleArn\": role_arn\n",
    "                                                              } \n",
    "                                                          },\n",
    "                                                          TimestampFormat=TIMESTAMP_FORMAT\n",
    "                                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:forecast:ap-northeast-2:180246855724:dataset-import-job/forecast_poc_cwk_ds_related/DSIMPORT_JOB_RELATEDPOC\n"
     ]
    }
   ],
   "source": [
    "rel_ds_import_job_arn=related_ds_import_job_response['DatasetImportJobArn']\n",
    "print(rel_ds_import_job_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will poll until the import process has completed, once that has been accomplished we can review the metrics and decide to delete the data or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE_IN_PROGRESS\n",
      "CREATE_IN_PROGRESS\n",
      "CREATE_IN_PROGRESS\n",
      "CREATE_IN_PROGRESS\n",
      "CREATE_IN_PROGRESS\n",
      "CREATE_IN_PROGRESS\n",
      "CREATE_IN_PROGRESS\n",
      "CREATE_IN_PROGRESS\n",
      "CREATE_IN_PROGRESS\n",
      "ACTIVE\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    dataImportStatus = forecast.describe_dataset_import_job(DatasetImportJobArn=rel_ds_import_job_arn)['Status']\n",
    "    print(dataImportStatus)\n",
    "    if dataImportStatus != 'ACTIVE' and dataImportStatus != 'CREATE_FAILED':\n",
    "        sleep(30)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**위의 작업 후에 아래와 같이 TARGET_TIME_SERIES, RELATED_TIME_SERIES가 보여야 함.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RelationDataSet](static/imgs/RelationDataSet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Related Time Series Data\n",
    "\n",
    "First let us examine the dataframe that we provided to Forecast:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 11433 entries, 2017-09-01 00:00:00 to 2018-09-30 23:00:00\n",
      "Data columns (total 6 columns):\n",
      "temp                   11433 non-null float64\n",
      "rain_1h                11433 non-null float64\n",
      "snow_1h                11433 non-null float64\n",
      "clouds_all             11433 non-null float64\n",
      "weather_description    11433 non-null object\n",
      "item_ID                11433 non-null object\n",
      "dtypes: float64(4), object(2)\n",
      "memory usage: 945.2+ KB\n"
     ]
    }
   ],
   "source": [
    "related_time_series_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temp</th>\n",
       "      <th>rain_1h</th>\n",
       "      <th>snow_1h</th>\n",
       "      <th>clouds_all</th>\n",
       "      <th>weather_description</th>\n",
       "      <th>item_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-08-01 07:00:00</th>\n",
       "      <td>292.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>proximity thunderstorm</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-20 20:00:00</th>\n",
       "      <td>274.86</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>broken clouds</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-31 08:00:00</th>\n",
       "      <td>294.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>proximity thunderstorm</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       temp  rain_1h  snow_1h  clouds_all  \\\n",
       "2018-08-01 07:00:00  292.01     0.00      0.0        90.0   \n",
       "2018-01-20 20:00:00  274.86     0.00      0.0        75.0   \n",
       "2018-08-31 08:00:00  294.00     0.25      0.0        75.0   \n",
       "\n",
       "                        weather_description item_ID  \n",
       "2018-08-01 07:00:00  proximity thunderstorm       1  \n",
       "2018-01-20 20:00:00           broken clouds       1  \n",
       "2018-08-31 08:00:00  proximity thunderstorm       1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "related_time_series_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we see 18,609 entries and not one is a NaN value! This is perfect. Now to double check what we imported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DatasetImportJobName': 'DSIMPORT_JOB_RELATEDPOC',\n",
       " 'DatasetImportJobArn': 'arn:aws:forecast:ap-northeast-2:180246855724:dataset-import-job/forecast_poc_cwk_ds_related/DSIMPORT_JOB_RELATEDPOC',\n",
       " 'DatasetArn': 'arn:aws:forecast:ap-northeast-2:180246855724:dataset/forecast_poc_cwk_ds_related',\n",
       " 'TimestampFormat': 'yyyy-MM-dd hh:mm:ss',\n",
       " 'DataSource': {'S3Config': {'Path': 's3://180246855724forecastpoc-cwk/related_time_series.csv',\n",
       "   'RoleArn': 'arn:aws:iam::180246855724:role/ForecastRolePOC'}},\n",
       " 'FieldStatistics': {'clouds_all': {'Count': 11433,\n",
       "   'CountDistinct': 19,\n",
       "   'CountNull': 0,\n",
       "   'CountNan': 0,\n",
       "   'Min': '0.0',\n",
       "   'Max': '92.0',\n",
       "   'Avg': 47.48456223213505,\n",
       "   'Stddev': 39.51154210949013},\n",
       "  'item_id': {'Count': 11433, 'CountDistinct': 1, 'CountNull': 0},\n",
       "  'rain_1h': {'Count': 11433,\n",
       "   'CountDistinct': 87,\n",
       "   'CountNull': 0,\n",
       "   'CountNan': 0,\n",
       "   'Min': '0.0',\n",
       "   'Max': '10.6',\n",
       "   'Avg': 0.08465931951368846,\n",
       "   'Stddev': 0.5234969401681563},\n",
       "  'snow_1h': {'Count': 11433,\n",
       "   'CountDistinct': 1,\n",
       "   'CountNull': 0,\n",
       "   'CountNan': 0,\n",
       "   'Min': '0.0',\n",
       "   'Max': '0.0',\n",
       "   'Avg': 0.0,\n",
       "   'Stddev': 0.0},\n",
       "  'temperature': {'Count': 11433,\n",
       "   'CountDistinct': 3156,\n",
       "   'CountNull': 0,\n",
       "   'CountNan': 0,\n",
       "   'Min': '246.15',\n",
       "   'Max': '310.07',\n",
       "   'Avg': 281.54932502405313,\n",
       "   'Stddev': 13.185630831128977},\n",
       "  'timestamp': {'Count': 11433,\n",
       "   'CountDistinct': 9480,\n",
       "   'CountNull': 0,\n",
       "   'Min': '2017-09-01T00:00:00Z',\n",
       "   'Max': '2018-09-30T23:00:00Z'},\n",
       "  'weather': {'Count': 11433, 'CountDistinct': 32, 'CountNull': 0}},\n",
       " 'DataSize': 0.0005747666582465172,\n",
       " 'Status': 'ACTIVE',\n",
       " 'CreationTime': datetime.datetime(2020, 3, 22, 11, 58, 38, 670000, tzinfo=tzlocal()),\n",
       " 'LastModificationTime': datetime.datetime(2020, 3, 22, 12, 3, 4, 662000, tzinfo=tzlocal()),\n",
       " 'ResponseMetadata': {'RequestId': '14bd1f62-6383-4658-abc6-237ea4bd312e',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'content-type': 'application/x-amz-json-1.1',\n",
       "   'date': 'Sun, 22 Mar 2020 12:31:11 GMT',\n",
       "   'x-amzn-requestid': '14bd1f62-6383-4658-abc6-237ea4bd312e',\n",
       "   'content-length': '1439',\n",
       "   'connection': 'keep-alive'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast.describe_dataset_import_job(DatasetImportJobArn=rel_ds_import_job_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastict! No NaNs or nulls and the entire dataset is ready to go. If you'd like to delete this information so you can build your models without related data simply uncomment the cell below. Once that is done you are ready to move forward building your models with Amazon Forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forecast.delete_dataset_import_job(DatasetImportJobArn=rel_ds_import_job_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
